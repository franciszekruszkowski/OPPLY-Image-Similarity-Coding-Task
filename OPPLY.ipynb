{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/franciszekruszkowski/Desktop/OPPLY'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "class CLIPFeatureExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        model_name = \"openai/clip-vit-base-patch32\"\n",
    "        self.model = CLIPModel.from_pretrained(model_name)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_text(self, text):\n",
    "        \n",
    "        # input text\n",
    "        # torch tensor: (1,512) vector of text embedding\n",
    "        \n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(self.device)\n",
    "        \n",
    "        text_features = self.model.get_text_features(**inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return text_features\n",
    "   \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_images(self, images):\n",
    "        \n",
    "        # input images\n",
    "        # torch_tensor : (n,1,512) vector of image features\n",
    "        \n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(self.device)\n",
    "        \n",
    "        image_features = self.model.get_image_features(**inputs)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = CLIPFeatureExtractor()\n",
    "almonds_text_embedding = ft.encode_text('almonds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(fname):\n",
    "    img = Image.open(fname)\n",
    "    return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_for_directory(directory):\n",
    "    \n",
    "    image_files = os.listdir(directory)\n",
    "    image_files = [os.path.join(directory, f) for f in image_files]\n",
    "    \n",
    "    images = [load_image(f) for f in image_files]\n",
    "    \n",
    "    image_features = ft.encode_images(images)\n",
    "    \n",
    "    similarities = [util.cos_sim(img, almonds_text_embedding).item() for img in image_features]\n",
    "    \n",
    "    return np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds_for_examples(res_path='examples/'):\n",
    "    \n",
    "    irrelevant_dir = os.path.join(res_path, 'Irrelevant')\n",
    "    relevant_dir = os.path.join(res_path, 'Relevant')\n",
    "\n",
    "    irrelevant_examples = [os.path.join(irrelevant_dir, example) for example in os.listdir(irrelevant_dir)]\n",
    "    relevant_examples = [os.path.join(relevant_dir, example) for example in os.listdir(relevant_dir)]\n",
    "    \n",
    "    examples = irrelevant_examples + relevant_examples\n",
    "    \n",
    "    preds = [make_predictions_for_directory(example) for example in examples]\n",
    "    preds_dict = {example: pred for example, pred in zip(examples, preds)}\n",
    "    \n",
    "    \n",
    "    ### Scaled \n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    preds_scaled = scaler.fit_transform(np.array([preds]).reshape(-1,1))\n",
    "    preds_dict_scaled = {example: pred for example, pred in zip(examples, preds_scaled)}\n",
    "    \n",
    "    return preds_dict, preds_dict_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds_for_google_images(res_path='google_images/'):\n",
    "    \n",
    "    image_dirs = os.listdir(res_path)\n",
    "    image_dir_paths = [os.path.join(res_path, image_dir) for image_dir in image_dirs]\n",
    "    \n",
    "    labels = [img_dir.lower() for img_dir in image_dirs]\n",
    "    \n",
    "    avg_similarities = [make_predictions_for_directory(directory) for directory in (image_dir_paths)]\n",
    "    \n",
    "    preds_dict = {label: sim for label, sim in zip(labels, avg_similarities)}\n",
    "    \n",
    "    return preds_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for examples: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'examples/Irrelevant/Example 1': 0.16390372753143312,\n",
       " 'examples/Irrelevant/Example 3': 0.20035310144777652,\n",
       " 'examples/Irrelevant/Example 2': 0.20734971463680268,\n",
       " 'examples/Relevant/Example 1': 0.23983665242791175,\n",
       " 'examples/Relevant/Example 3': 0.24710791371762753,\n",
       " 'examples/Relevant/Example 2': 0.24388677423650568}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Making predictions for examples: ')\n",
    "examples_dic,   preds_scaled = make_preds_for_examples()\n",
    "examples_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for examples - MinMax SCALED: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'examples/Irrelevant/Example 1': array([0.]),\n",
       " 'examples/Irrelevant/Example 3': array([0.4380714]),\n",
       " 'examples/Irrelevant/Example 2': array([0.52216107]),\n",
       " 'examples/Relevant/Example 1': array([0.91260943]),\n",
       " 'examples/Relevant/Example 3': array([1.]),\n",
       " 'examples/Relevant/Example 2': array([0.96128633])}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Making predictions for examples - MinMax SCALED: ')\n",
    "preds_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for google images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franciszekruszkowski/opt/anaconda3/lib/python3.7/site-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'maple syrup': 0.18799603870138526,\n",
       " 'dates': 0.2504647237062454,\n",
       " 'cocoa': 0.22287209704518318,\n",
       " 'coconut': 0.2238942816549418,\n",
       " 'peanuts': 0.2437329798936844,\n",
       " 'oats': 0.21118771225214006,\n",
       " 'almonds': 0.30445924401283264,\n",
       " 'hazelnuts': 0.2631231632828712,\n",
       " 'random': 0.18072939336299895,\n",
       " 'sunflower oil': 0.1849773100444249}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Making predictions for google images')\n",
    "google_images_dic = make_preds_for_google_images()\n",
    "google_images_dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
